---
---

@string{tacl = {Transactions of the Association for Computational Linguistics (TACL)}}
@string{emnlp = {Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{acl = {Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{emnlpfindings = {Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{aclfindings = {Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{icml = {International Conference on Machine Learning (ICML)}}
@string{tmlr = {Transactions on Machine Learning Research (TMLR)}}
@string{ijcnlp-aacl = {International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)}}
@string{aacl = {Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL)}}
@string{ijcnlp = {International Joint Conference on Natural Language Processing (IJCNLP)}}
@string{www = {International World Wide Web Conference (WWW)}}
@string{aaai = {AAAI Conference on Artificial Intelligence (AAAI)}}
@string{naacl = {Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}}
@string{coling = {International Conference on Computational Linguistics (COLING)}}
@string{icmr = {International Conference on Multimedia Retrieval (ICMR)}}
@string{acmmm = {ACM International Conference on Multimedia (ACM MM)}}
@string{cvpr = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string{iccv = {IEEE/CVF International Conference on Computer Vision (ICCV)}}
@string{tmm = {IEEE Transactions on Multimedia (TMM)}}

@article{DBLP:journals/corr/abs-2506-11930,
  abbr = {NeurIPS},
  author       = {Dongwei Jiang and
                  Alvin Zhang and
                  Andrew Wang and
                  Nicholas Andrews and
                  Daniel Khashabi},
  title        = {Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback},
  journal      = {CoRR},
  volume       = {abs/2506.11930},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2506.11930},
  doi          = {10.48550/ARXIV.2506.11930},
  eprinttype    = {arXiv},
  eprint       = {2506.11930},
  timestamp    = {Thu, 10 Jul 2025 13:08:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2506-11930.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  selected = {true},
  pdf={https://arxiv.org/abs/2506.11930},
  abstract = "Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and reach correct solutions. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 with extended thinking. Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limita- tion, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We analyze FEEDBACK FRICTION and find that models' confidence on specific questions, measured by semantic entropy, predicts feedback resistance: high-confidence predictions remain resistant to external correction. We hope that highlighting this issue in LLMs will help future research in self-improvement."
}

@inproceedings{jiang-etal-2025-rationalyst,
    abbr = {ACL},
    title = "{RATIONALYST}: Pre-training Process-Supervision for Improving Reasoning",
    author = "Jiang, Dongwei  and
      Wang, Guoxuan  and
      Lu, Yining  and
      Wang, Andrew  and
      Zhang, Jingyu  and
      Liu, Chuyu  and
      Van Durme, Benjamin  and
      Khashabi, Daniel",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1288/",
    doi = "10.18653/v1/2025.acl-long.1288",
    pages = "26547--26566",
    ISBN = "979-8-89176-251-0",
    selected = {true},
    pdf = "https://aclanthology.org/2025.acl-long.1288/",
    abstract = "The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets."
}

@inproceedings{DBLP:conf/iclr/SpragueYRJWSZYM25,
  abbr = {ICLR},
  author       = {Zayne Rea Sprague and
                  Fangcong Yin and
                  Juan Diego Rodriguez and
                  Dongwei Jiang and
                  Manya Wadhwa and
                  Prasann Singhal and
                  Xinyu Zhao and
                  Xi Ye and
                  Kyle Mahowald and
                  Greg Durrett},
  title        = {To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic
                  reasoning},
  booktitle    = {The Thirteenth International Conference on Learning Representations,
                  {ICLR} 2025, Singapore, April 24-28, 2025},
  publisher    = {OpenReview.net},
  year         = {2025},
  url          = {https://openreview.net/forum?id=w6nlcS8Kkn},
  timestamp    = {Thu, 15 May 2025 17:19:06 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SpragueYRJWSZYM25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  selected = {true},
  pdf={https://arxiv.org/abs/2409.12183},
  abstract = "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications."
}

@inproceedings{DBLP:conf/naacl/JiangFC24,
  abbr = {NAACL},
  author       = {Dongwei Jiang and Marcio Fonseca and Shay B. Cohen},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {LeanReasoner: Boosting Complex Logical Reasoning with Lean},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {7497--7510},
  selected = {true},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.416},
  doi          = {10.18653/V1/2024.NAACL-LONG.416},
  timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/JiangFC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems into theorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean's symbolic solver. It also enhances our ability to treat complex reasoning tasks by using Lean's extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset.",
  pdf={https://arxiv.org/abs/2403.13312}
}

@inproceedings{weir-etal-2024-enhancing,
    abbr = {EMNLP},
    title = "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic",
    author = "Weir, Nathaniel  and
      Sanders, Kate  and
      Weller, Orion  and
      Sharma, Shreya  and
      Jiang, Dongwei  and
      Jiang, Zhengping  and
      Dalvi Mishra, Bhavana  and
      Tafjord, Oyvind  and
      Jansen, Peter  and
      Clark, Peter  and
      Van Durme, Benjamin",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.531",
    pdf = "https://aclanthology.org/2024.emnlp-main.531",
    pages = "9458--9482",
    selected = {true},
    abstract = "Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what {\_}valid decompositional entailment{\_} is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic entailment engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference.",
}


@article{DBLP:journals/corr/abs-2404-04298,
  author       = {Dongwei Jiang and
                  Jingyu Zhang and
                  Orion Weller and
                  Nathaniel Weir and
                  Benjamin Van Durme and
                  Daniel Khashabi},
  abbr = {AAAI},
  title        = {{SELF-[IN]CORRECT:} LLMs Struggle with Discriminating Self-Generated Responses},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.04298},
  selected = {true},
  doi          = {10.48550/ARXIV.2404.04298},
  timestamp    = {Wed, 15 May 2024 08:47:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-04298.bib},
  pdf = "https://arxiv.org/abs/2404.04298",
  abstract = "Can LLMs consistently improve their previous outputs for better results? For this to be true, LLMs would need to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first formulate a unified framework that allows us to compare the generative and discriminative capability of any model on any task. In our resulting experimental analysis of several open-source and industrial LLMs, we observe that models are not reliably better at discriminating among previously-generated alternatives than generating initial responses. This finding challenges the notion that LLMs may be able to enhance their performance only through their own judgment."
}

@article{DBLP:journals/corr/abs-2407-09007,
  abbr = {NAACL},
  author       = {Yining Lu and
                  Dixuan Wang and
                  Tianjian Li and
                  Dongwei Jiang and
                  Daniel Khashabi},
  title        = {Benchmarking Language Model Creativity: {A} Case Study on Code Generation},
  journal      = {CoRR},
  volume       = {abs/2407.09007},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.09007},
  doi          = {10.48550/ARXIV.2407.09007},
  eprinttype    = {arXiv},
  eprint       = {2407.09007},
  timestamp    = {Thu, 15 Aug 2024 11:20:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-09007.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = "https://arxiv.org/abs/2407.09007",
  abstract = "As LLMs become increasingly prevalent, it is interesting to consider how ``creative'' these models can be. From cognitive science, creativity consists of at least two key characteristics: \emph{convergent} thinking (purposefulness to achieve a given goal) and \emph{divergent} thinking (adaptability to new environments or constraints) \citep{runco2003critical}. In this work, we introduce a framework for quantifying LLM creativity that incorporates the two characteristics. This is achieved by (1) Denial Prompting pushes LLMs to come up with more creative solutions to a given problem by incrementally imposing new constraints on the previous solution, compelling LLMs to adopt new strategies, and (2) defining and computing the NeoGauge metric which examines both convergent and divergent thinking in the generated creative responses by LLMs. We apply the proposed framework on Codeforces problems, a natural data source for collecting human coding solutions. We quantify NeoGauge for various proprietary and open-source models and find that even the most creative model, GPT-4, still falls short of demonstrating human-like creativity. We also experiment with advanced reasoning strategies (MCTS, self-correction, etc.) and observe no significant improvement in creativity. As a by-product of our analysis, we release NeoCoder dataset for reproducing our results on future models.",
}

@inproceedings{DBLP:conf/icassp/JiangLZCLHZHL21,
  abbr = {ICASSP},
  author       = {Dongwei Jiang and
                  Wubo Li and
                  Ruixiong Zhang and
                  Miao Cao and
                  Ne Luo and
                  Yang Han and
                  Wei Zou and
                  Kun Han and
                  Xiangang Li},
  title        = {A Further Study of Unsupervised Pretraining for Transformer Based
                  Speech Recognition},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  selected = {true},
  pages        = {6538--6542},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414539},
  doi          = {10.1109/ICASSP39728.2021.9414539},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/JiangLZCLHZHL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.",
  pdf = "https://arxiv.org/abs/2005.09862",
}

@inproceedings{DBLP:conf/icassp/ZhangWLJZL21,
  abbr = {ICASSP},
  author       = {Ruixiong Zhang and
                  Haiwei Wu and
                  Wubo Li and
                  Dongwei Jiang and
                  Wei Zou and
                  Xiangang Li},
  title        = {Transformer Based Unsupervised Pre-Training for Acoustic Representation
                  Learning},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages        = {6933--6937},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414996},
  doi          = {10.1109/ICASSP39728.2021.9414996},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/ZhangWLJZL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Recently, a variety of acoustic tasks and related applications arised. For many acoustic tasks, the labeled data size may be limited. To handle this problem, we propose an unsupervised pre-training method using Transformer based encoder to learn a general and robust high-level representation for all acoustic tasks. Experiments have been conducted on three kinds of acoustic tasks: speech emotion recognition, sound event detection and speech translation. All the experiments have shown that pre-training using its own training data can significantly improve the performance. With a larger pre-training data combining MuST-C, Librispeech and ESC-US datasets, for speech emotion recognition, the UAR can further improve absolutely 4.3% on IEMOCAP dataset. For sound event detection, the F1 score can further improve absolutely 1.5% on DCASE2018 task5 development set and 2.1% on evaluation set. For speech translation, the BLEU score can further improve relatively 12.2% on En-De dataset and 8.4% on En-Fr dataset.",
  pdf = "https://arxiv.org/abs/2007.14602",
}

@inproceedings{DBLP:conf/icassp/GuoWJLZZLGZHL21,
  abbr = {InterSpeech},
  author       = {Tingwei Guo and
                  Cheng Wen and
                  Dongwei Jiang and
                  Ne Luo and
                  Ruixiong Zhang and
                  Shuaijiang Zhao and
                  Wubo Li and
                  Cheng Gong and
                  Wei Zou and
                  Kun Han and
                  Xiangang Li},
  title        = {Didispeech: {A} Large Scale Mandarin Speech Corpus},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages        = {6968--6972},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414423},
  doi          = {10.1109/ICASSP39728.2021.9414423},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/GuoWJLZZLGZHL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "This paper introduces a new open-sourced Mandarin speech corpus, called DiDiSpeech. It consists of about 800 hours of speech data at 48kHz sampling rate from 6000 speakers and the corresponding texts. All speech data in the corpus is recorded in quiet environment and is suitable for various speech processing tasks, such as voice conversion, multi-speaker text-to-speech and automatic speech recognition. We conduct experiments with multiple speech tasks and evaluate the performance, showing that it is promising to use the corpus for both academic research and practical application. The corpus is available at this https URL.",
  pdf = "https://arxiv.org/abs/2010.09275",
}

@inproceedings{DBLP:conf/interspeech/JiangLCZL21,
  abbr = {InterSpeech},
  author       = {Dongwei Jiang and
                  Wubo Li and
                  Miao Cao and
                  Wei Zou and
                  Xiangang Li},
  editor       = {Hynek Hermansky and
                  Honza Cernock{\'{y}} and
                  Luk{\'{a}}s Burget and
                  Lori Lamel and
                  Odette Scharenborg and
                  Petr Motl{\'{\i}}cek},
  title        = {Speech SimCLR: Combining Contrastive and Reconstruction Objective
                  for Self-Supervised Speech Representation Learning},
  booktitle    = {22nd Annual Conference of the International Speech Communication Association,
                  Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021},
  pages        = {1544--1548},
  publisher    = {{ISCA}},
  selected = {true},
  year         = {2021},
  url          = {https://doi.org/10.21437/Interspeech.2021-391},
  doi          = {10.21437/INTERSPEECH.2021-391},
  timestamp    = {Tue, 11 Jun 2024 16:45:43 +0200},
  biburl       = {https://dblp.org/rec/conf/interspeech/JiangLCZL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Self-supervised visual pretraining has shown significant progress recently. Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semi-supervised learning on ImageNet. The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning. In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning. During training, Speech SimCLR applies augmentation on raw speech and its spectrogram. Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation. The proposed method achieved competitive results on speech emotion recognition and speech recognition.",
  pdf = "https://arxiv.org/abs/2010.13991",
}

@inproceedings{DBLP:conf/interspeech/LiJZL20,
  abbr = {InterSpeech},
  author       = {Wubo Li and
                  Dongwei Jiang and
                  Wei Zou and
                  Xiangang Li},
  editor       = {Helen Meng and
                  Bo Xu and
                  Thomas Fang Zheng},
  title        = {{TMT:} {A} Transformer-Based Modal Translator for Improving Multimodal
                  Sequence Representations in Audio Visual Scene-Aware Dialog},
  booktitle    = {21st Annual Conference of the International Speech Communication Association,
                  Interspeech 2020, Virtual Event, Shanghai, China, October 25-29, 2020},
  pages        = {3501--3505},
  publisher    = {{ISCA}},
  year         = {2020},
  url          = {https://doi.org/10.21437/Interspeech.2020-2359},
  doi          = {10.21437/INTERSPEECH.2020-2359},
  timestamp    = {Tue, 11 Jun 2024 16:45:43 +0200},
  biburl       = {https://dblp.org/rec/conf/interspeech/LiJZL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses when discussing about a given video. The previous state-of-the-art model shows superior performance for this task using Transformer-based architecture. However, there remain some limitations in learning better representation of modalities. Inspired by Neural Machine Translation (NMT), we propose the Transformer-based Modal Translator (TMT) to learn the representations of the source modal sequence by translating the source modal sequence to the related target modal sequence in a supervised manner. Based on Multimodal Transformer Networks (MTN), we apply TMT to video and dialog, proposing MTN-TMT for the video-grounded dialog system. On the AVSD track of the Dialog System Technology Challenge 7, MTN-TMT outperforms the MTN and other submission models in both Video and Text task and Text Only task. Compared with MTN, MTN-TMT improves all metrics, especially, achieving relative improvement up to 14.1% on CIDEr. Index Terms: multimodal learning, audio-visual scene-aware dialog, neural machine translation, multi-task learning",
  pdf = "https://arxiv.org/abs/2010.10839",
}

@inproceedings{DBLP:conf/iscslp/ZouJZYL18,
  abbr = {ISCSLP},
  author       = {Wei Zou and
                  Dongwei Jiang and
                  Shuaijiang Zhao and
                  Guilin Yang and
                  Xiangang Li},
  title        = {Comparable Study Of Modeling Units For End-To-End Mandarin Speech
                  Recognition},
  booktitle    = {11th International Symposium on Chinese Spoken Language Processing,
                  {ISCSLP} 2018, Taipei City, Taiwan, November 26-29, 2018},
  pages        = {369--373},
  publisher    = {{IEEE}},
  year         = {2018},
  url          = {https://doi.org/10.1109/ISCSLP.2018.8706661},
  doi          = {10.1109/ISCSLP.2018.8706661},
  timestamp    = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iscslp/ZouJZYL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = "https://arxiv.org/abs/1805.03832",
  abstract = "End-To-End speech recognition have become increasingly popular in mandarin speech recognition and achieved delightful performance. Mandarin is a tonal language which is different from English and requires special treatment for the acoustic modeling units. There have been several different kinds of modeling units for mandarin such as phoneme, syllable and Chinese character. In this work, we explore two major end-to-end models: connectionist temporal classification (CTC) model and attention based encoder-decoder model for mandarin speech recognition. We compare the performance of three different scaled modeling units: context dependent phoneme(CDP), syllable with tone and Chinese character. We find that all types of modeling units can achieve approximate character error rate (CER) in CTC model and the performance of Chinese character attention model is better than syllable attention model. Furthermore, we find that Chinese character is a reasonable unit for mandarin speech recognition. On DidiCallcenter task, Chinese character attention model achieves a CER of 5.68% and CTC model gets a CER of 7.29%, on the other DidiReading task, CER are 4.89% and 5.79%, respectively. Moreover, attention model achieves a better performance than CTC model on both datasets.",
}

@inproceedings{DBLP:conf/iscslp/JiangZZYL18,
  abbr = {ISCSLP},
  author       = {Dongwei Jiang and
                  Wei Zou and
                  Shuaijiang Zhao and
                  Guilin Yang and
                  Xiangang Li},
  title        = {An Analysis of Decoding for Attention-Based End-to-End Mandarin Speech
                  Recognition},
  booktitle    = {11th International Symposium on Chinese Spoken Language Processing,
                  {ISCSLP} 2018, Taipei City, Taiwan, November 26-29, 2018},
  pages        = {384--388},
  publisher    = {{IEEE}},
  year         = {2018},
  url          = {https://doi.org/10.1109/ISCSLP.2018.8706686},
  doi          = {10.1109/ISCSLP.2018.8706686},
  timestamp    = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iscslp/JiangZZYL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706686",
  abstract = "Many of the current state-of-the-art Mandarin Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are built either with a hybrid of Deep Neural Network (DNN) and Hidden Markov Models (HMM) or with Neural Network model trained with Connectionist Temporal Classification (CTC) criterion. In both of these models, decoding is conducted by Weighted Finite State Transducer (WFST) that searches the word sequence which matches best with the speech given the acoustic and language models. Recently, attention-based end-toend method is becoming more and more popular for Mandarin speech recognition. This new method advocates replacing complex data processing pipelines with a single neural network trained in an end-to-end fashion. In this paper, we investigate the decoding process for attention-based Mandarin models using syllable and character as acoustic modeling units and discuss how to combine word information into the decoding process. We also conduct a detailed analysis on various factors that affect the performance of decoding including beam size, label smoothing, softmax temperature, attention smoothing and coverage."},

@article{DBLP:journals/corr/abs-1810-13091,
  abbr = {arxiv},
  author       = {Ne Luo and
                  Dongwei Jiang and
                  Shuaijiang Zhao and
                  Caixia Gong and
                  Wei Zou and
                  Xiangang Li},
  title        = {Towards End-to-End Code-Switching Speech Recognition},
  journal      = {CoRR},
  volume       = {abs/1810.13091},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.13091},
  eprinttype    = {arXiv},
  eprint       = {1810.13091},
  timestamp    = {Thu, 08 Nov 2018 10:57:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-13091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Code-switching speech recognition has attracted an increasing interest recently, but the need for expert linguistic knowledge has always been a big issue. End-to-end automatic speech recognition (ASR) simplifies the building of ASR systems considerably by predicting graphemes or characters directly from acoustic input. In the mean time, the need of expert linguistic knowledge is also eliminated, which makes it an attractive choice for code-switching ASR. This paper presents a hybrid CTC-Attention based end-to-end Mandarin-English code-switching (CS) speech recognition system and studies the effect of hybrid CTC-Attention based models, different modeling units, the inclusion of language identification and different decoding strategies on the task of code-switching ASR. On the SEAME corpus, our system achieves a mixed error rate (MER) of 34.24%.",
  pdf = "https://arxiv.org/abs/1810.13091",
}

@article{DBLP:journals/corr/abs-1910-09932,
  abbr = {arxiv},
  author       = {Dongwei Jiang and
                  Xiaoning Lei and
                  Wubo Li and
                  Ne Luo and
                  Yuxuan Hu and
                  Wei Zou and
                  Xiangang Li},
  title        = {Improving Transformer-based Speech Recognition Using Unsupervised
                  Pre-training},
  journal      = {CoRR},
  volume       = {abs/1910.09932},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.09932},
  eprinttype    = {arXiv},
  eprint       = {1910.09932},
  timestamp    = {Fri, 25 Oct 2019 14:59:26 +0200},
  selected = {true},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-09932.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3%, exceeding the best end-to-end model by over 0.2% absolute CER. With more pre-training data, we can further reduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.",
  pdf = "https://arxiv.org/abs/1910.09932"
}
