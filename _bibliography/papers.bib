---
---

@string{tacl = {Transactions of the Association for Computational Linguistics (TACL)}}
@string{emnlp = {Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{acl = {Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{emnlpfindings = {Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{aclfindings = {Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{icml = {International Conference on Machine Learning (ICML)}}
@string{tmlr = {Transactions on Machine Learning Research (TMLR)}}
@string{ijcnlp-aacl = {International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)}}
@string{aacl = {Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (AACL)}}
@string{ijcnlp = {International Joint Conference on Natural Language Processing (IJCNLP)}}
@string{www = {International World Wide Web Conference (WWW)}}
@string{aaai = {AAAI Conference on Artificial Intelligence (AAAI)}}
@string{naacl = {Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}}
@string{coling = {International Conference on Computational Linguistics (COLING)}}
@string{icmr = {International Conference on Multimedia Retrieval (ICMR)}}
@string{acmmm = {ACM International Conference on Multimedia (ACM MM)}}
@string{cvpr = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string{iccv = {IEEE/CVF International Conference on Computer Vision (ICCV)}}
@string{tmm = {IEEE Transactions on Multimedia (TMM)}}

@misc{ma2024mmlongbenchdocbenchmarkinglongcontextdocument,
    abbr = {arXiv},
    dataset={https://huggingface.co/datasets/yubo2333/MMLongBench-Doc},
    code={https://github.com/mayubo2333/MMLongBench-Doc},
    website={https://mayubo2333.github.io/MMLongBench-Doc/},
    pdf={https://arxiv.org/abs/2407.01523},
    abstract = "Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLongBench-Doc, a long-context, multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct from previous datasets, it is constructed upon 130 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e. page number). Moreover, 33.2% of the questions are cross-page questions requiring evidence across multiple pages. 22.8% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",
    bibtex_show = {true},
    title={MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations}, 
    author={Yubo Ma and Yuhang Zang and Liangyu Chen and Meiqi Chen and Yizhu Jiao and Xinze Li and Xinyuan Lu and Ziyu Liu and Yan Ma and Xiaoyi Dong and Pan Zhang and Liangming Pan and Yu-Gang Jiang and Jiaqi Wang and Yixin Cao and Aixin Sun},
    year={2024},
    eprint={2407.01523},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2407.01523}
}

@inproceedings{DBLP:conf/naacl/JiangFC24,
  abbr = {NAACL},
  author       = {Dongwei Jiang and Marcio Fonseca and Shay B. Cohen},
  editor       = {Kevin Duh and
                  Helena G{\'{o}}mez{-}Adorno and
                  Steven Bethard},
  title        = {LeanReasoner: Boosting Complex Logical Reasoning with Lean},
  booktitle    = {Proceedings of the 2024 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies
                  (Volume 1: Long Papers), {NAACL} 2024, Mexico City, Mexico, June 16-21,
                  2024},
  pages        = {7497--7510},
  selected = {true},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.naacl-long.416},
  doi          = {10.18653/V1/2024.NAACL-LONG.416},
  timestamp    = {Thu, 29 Aug 2024 17:13:57 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/JiangFC24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems into theorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean's symbolic solver. It also enhances our ability to treat complex reasoning tasks by using Lean's extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset.",
  pdf={https://arxiv.org/abs/2403.13312}
}

@article{DBLP:journals/corr/abs-2402-14798,
  abbr = {arXiv},
  author       = {Nathaniel Weir and
                  Kate Sanders and
                  Orion Weller and
                  Shreya Sharma and
                  Dongwei Jiang and
                  Zhengping Jiang and
                  Bhavana Dalvi Mishra and
                  Oyvind Tafjord and
                  Peter A. Jansen and
                  Peter Clark and
                  Benjamin Van Durme},
  title        = {Enhancing Systematic Decompositional Natural Language Inference Using
                  Informal Logic},
  journal      = {CoRR},
  volume       = {abs/2402.14798},
  year         = {2024},
  selected = {true},
  url          = {https://doi.org/10.48550/arXiv.2402.14798},
  doi          = {10.48550/ARXIV.2402.14798},
  eprinttype    = {arXiv},
  eprint       = {2402.14798},
  timestamp    = {Wed, 10 Apr 2024 15:48:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-14798.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference.",
  pdf = "https://arxiv.org/abs/2402.14798"
}

@article{DBLP:journals/corr/abs-2404-04298,
  author       = {Dongwei Jiang and
                  Jingyu Zhang and
                  Orion Weller and
                  Nathaniel Weir and
                  Benjamin Van Durme and
                  Daniel Khashabi},
  abbr = {arXiv},
  title        = {{SELF-[IN]CORRECT:} LLMs Struggle with Refining Self-Generated Responses},
  journal      = {CoRR},
  volume       = {abs/2404.04298},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.04298},
  selected = {true},
  doi          = {10.48550/ARXIV.2404.04298},
  eprinttype    = {arXiv},
  eprint       = {2404.04298},
  timestamp    = {Wed, 15 May 2024 08:47:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-04298.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = "https://arxiv.org/abs/2404.04298"
}

@article{DBLP:journals/corr/abs-2407-09007,
  abbr = {arXiv},
  author       = {Yining Lu and
                  Dixuan Wang and
                  Tianjian Li and
                  Dongwei Jiang and
                  Daniel Khashabi},
  title        = {Benchmarking Language Model Creativity: {A} Case Study on Code Generation},
  journal      = {CoRR},
  volume       = {abs/2407.09007},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.09007},
  doi          = {10.48550/ARXIV.2407.09007},
  eprinttype    = {arXiv},
  eprint       = {2407.09007},
  timestamp    = {Thu, 15 Aug 2024 11:20:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-09007.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = "https://arxiv.org/abs/2407.09007"
}

@inproceedings{DBLP:conf/icassp/JiangLZCLHZHL21,
  abbr = {ICASSP},
  author       = {Dongwei Jiang and
                  Wubo Li and
                  Ruixiong Zhang and
                  Miao Cao and
                  Ne Luo and
                  Yang Han and
                  Wei Zou and
                  Kun Han and
                  Xiangang Li},
  title        = {A Further Study of Unsupervised Pretraining for Transformer Based
                  Speech Recognition},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  selected = {true},
  pages        = {6538--6542},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414539},
  doi          = {10.1109/ICASSP39728.2021.9414539},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/JiangLZCLHZHL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, many unsupervised pre-training methods have been proposed. Among these methods, Masked Predictive Coding achieved significant improvements on various speech recognition datasets with BERT-like Masked Reconstruction loss and Transformer backbone. However, many aspects of MPC have not been fully investigated. In this paper, we conduct a further study on MPC and focus on three important aspects: the effect of pre-training data speaking style, its extension on streaming model, and how to better transfer learned knowledge from pre-training stage to downstream tasks. Experiments reveled that pre-training data with matching speaking style is more useful on downstream recognition tasks. A unified training objective with APC and MPC provided 8.46% relative error reduction on streaming model trained on HKUST. Also, the combination of target data adaption and layer-wise discriminative training helped the knowledge transfer of MPC, which achieved 3.99% relative error reduction on AISHELL over a strong baseline.",
  pdf = "https://arxiv.org/abs/2005.09862"
}

@inproceedings{DBLP:conf/icassp/ZhangWLJZL21,
  abbr = {ICASSP},
  author       = {Ruixiong Zhang and
                  Haiwei Wu and
                  Wubo Li and
                  Dongwei Jiang and
                  Wei Zou and
                  Xiangang Li},
  title        = {Transformer Based Unsupervised Pre-Training for Acoustic Representation
                  Learning},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages        = {6933--6937},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414996},
  doi          = {10.1109/ICASSP39728.2021.9414996},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/ZhangWLJZL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icassp/GuoWJLZZLGZHL21,
  abbr = {InterSpeech},
  author       = {Tingwei Guo and
                  Cheng Wen and
                  Dongwei Jiang and
                  Ne Luo and
                  Ruixiong Zhang and
                  Shuaijiang Zhao and
                  Wubo Li and
                  Cheng Gong and
                  Wei Zou and
                  Kun Han and
                  Xiangang Li},
  title        = {Didispeech: {A} Large Scale Mandarin Speech Corpus},
  booktitle    = {{IEEE} International Conference on Acoustics, Speech and Signal Processing,
                  {ICASSP} 2021, Toronto, ON, Canada, June 6-11, 2021},
  pages        = {6968--6972},
  publisher    = {{IEEE}},
  year         = {2021},
  url          = {https://doi.org/10.1109/ICASSP39728.2021.9414423},
  doi          = {10.1109/ICASSP39728.2021.9414423},
  timestamp    = {Fri, 09 Jul 2021 13:04:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icassp/GuoWJLZZLGZHL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/interspeech/JiangLCZL21,
  abbr = {InterSpeech},
  author       = {Dongwei Jiang and
                  Wubo Li and
                  Miao Cao and
                  Wei Zou and
                  Xiangang Li},
  editor       = {Hynek Hermansky and
                  Honza Cernock{\'{y}} and
                  Luk{\'{a}}s Burget and
                  Lori Lamel and
                  Odette Scharenborg and
                  Petr Motl{\'{\i}}cek},
  title        = {Speech SimCLR: Combining Contrastive and Reconstruction Objective
                  for Self-Supervised Speech Representation Learning},
  booktitle    = {22nd Annual Conference of the International Speech Communication Association,
                  Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021},
  pages        = {1544--1548},
  publisher    = {{ISCA}},
  selected = {true},
  year         = {2021},
  url          = {https://doi.org/10.21437/Interspeech.2021-391},
  doi          = {10.21437/INTERSPEECH.2021-391},
  timestamp    = {Tue, 11 Jun 2024 16:45:43 +0200},
  biburl       = {https://dblp.org/rec/conf/interspeech/JiangLCZL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Self-supervised visual pretraining has shown significant progress recently. Among those methods, SimCLR greatly advanced the state of the art in self-supervised and semi-supervised learning on ImageNet. The input feature representations for speech and visual tasks are both continuous, so it is natural to consider applying similar objective on speech representation learning. In this paper, we propose Speech SimCLR, a new self-supervised objective for speech representation learning. During training, Speech SimCLR applies augmentation on raw speech and its spectrogram. Its objective is the combination of contrastive loss that maximizes agreement between differently augmented samples in the latent space and reconstruction loss of input representation. The proposed method achieved competitive results on speech emotion recognition and speech recognition.",
  pdf = "https://arxiv.org/abs/2010.13991"
}

@inproceedings{DBLP:conf/interspeech/LiJZL20,
  abbr = {InterSpeech},
  author       = {Wubo Li and
                  Dongwei Jiang and
                  Wei Zou and
                  Xiangang Li},
  editor       = {Helen Meng and
                  Bo Xu and
                  Thomas Fang Zheng},
  title        = {{TMT:} {A} Transformer-Based Modal Translator for Improving Multimodal
                  Sequence Representations in Audio Visual Scene-Aware Dialog},
  booktitle    = {21st Annual Conference of the International Speech Communication Association,
                  Interspeech 2020, Virtual Event, Shanghai, China, October 25-29, 2020},
  pages        = {3501--3505},
  publisher    = {{ISCA}},
  year         = {2020},
  url          = {https://doi.org/10.21437/Interspeech.2020-2359},
  doi          = {10.21437/INTERSPEECH.2020-2359},
  timestamp    = {Tue, 11 Jun 2024 16:45:43 +0200},
  biburl       = {https://dblp.org/rec/conf/interspeech/LiJZL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iscslp/ZouJZYL18,
  abbr = {ISCSLP},
  author       = {Wei Zou and
                  Dongwei Jiang and
                  Shuaijiang Zhao and
                  Guilin Yang and
                  Xiangang Li},
  title        = {Comparable Study Of Modeling Units For End-To-End Mandarin Speech
                  Recognition},
  booktitle    = {11th International Symposium on Chinese Spoken Language Processing,
                  {ISCSLP} 2018, Taipei City, Taiwan, November 26-29, 2018},
  pages        = {369--373},
  publisher    = {{IEEE}},
  year         = {2018},
  url          = {https://doi.org/10.1109/ISCSLP.2018.8706661},
  doi          = {10.1109/ISCSLP.2018.8706661},
  timestamp    = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iscslp/ZouJZYL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iscslp/JiangZZYL18,
  abbr = {ISCSLP},
  author       = {Dongwei Jiang and
                  Wei Zou and
                  Shuaijiang Zhao and
                  Guilin Yang and
                  Xiangang Li},
  title        = {An Analysis of Decoding for Attention-Based End-to-End Mandarin Speech
                  Recognition},
  booktitle    = {11th International Symposium on Chinese Spoken Language Processing,
                  {ISCSLP} 2018, Taipei City, Taiwan, November 26-29, 2018},
  pages        = {384--388},
  publisher    = {{IEEE}},
  year         = {2018},
  url          = {https://doi.org/10.1109/ISCSLP.2018.8706686},
  doi          = {10.1109/ISCSLP.2018.8706686},
  timestamp    = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iscslp/JiangZZYL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-13091,
  abbr = {arxiv},
  author       = {Ne Luo and
                  Dongwei Jiang and
                  Shuaijiang Zhao and
                  Caixia Gong and
                  Wei Zou and
                  Xiangang Li},
  title        = {Towards End-to-End Code-Switching Speech Recognition},
  journal      = {CoRR},
  volume       = {abs/1810.13091},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.13091},
  eprinttype    = {arXiv},
  eprint       = {1810.13091},
  timestamp    = {Thu, 08 Nov 2018 10:57:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-13091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1910-09932,
  abbr = {arxiv},
  author       = {Dongwei Jiang and
                  Xiaoning Lei and
                  Wubo Li and
                  Ne Luo and
                  Yuxuan Hu and
                  Wei Zou and
                  Xiangang Li},
  title        = {Improving Transformer-based Speech Recognition Using Unsupervised
                  Pre-training},
  journal      = {CoRR},
  volume       = {abs/1910.09932},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.09932},
  eprinttype    = {arXiv},
  eprint       = {1910.09932},
  timestamp    = {Fri, 25 Oct 2019 14:59:26 +0200},
  selected = {true},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-09932.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = "Speech recognition technologies are gaining enormous popularity in various industrial applications. However, building a good speech recognition system usually requires large amounts of transcribed data, which is expensive to collect. To tackle this problem, an unsupervised pre-training method called Masked Predictive Coding is proposed, which can be applied for unsupervised pre-training with Transformer based model. Experiments on HKUST show that using the same training data, we can achieve CER 23.3%, exceeding the best end-to-end model by over 0.2% absolute CER. With more pre-training data, we can further reduce the CER to 21.0%, or a 11.8% relative CER reduction over baseline.",
  pdf = "https://arxiv.org/abs/1910.09932"
}
