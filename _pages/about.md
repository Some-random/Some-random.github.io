---
layout: about
title: About
permalink: /
subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: left
  image: professional_reflection.jpg
  image_circular: true # crops the image to make it circular
  more_info: >
    <p>555 your office number</p>
    <p>123 your address street</p>
    <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I am broadly interested in **reasoning**, which (IMHO) is a key aspect of human intelligence that sets us apart from other species. In the realm of reasoning, I've worked on:
- **Building general-purpose verifier** through rationale extraction from unlabelled data to provide process supervision during reasoning
- **Logical reasoning** that uses theorem prover [Lean](https://lean-lang.org/) to help with the reasoning process [[2]](https://arxiv.org/abs/2403.13312)
- **Decompositional entailment** that formulates a consistent and theoretically grounded approach to annotating decompositional entailment dataset [[3]](https://arxiv.org/abs/2402.14798)

  
I'm also interested in the **self-improvement** capability of LLMs. If we begin with the “end” (superintelligence/AGI) in mind, relying on human input won't get us there. We need to teach models to interact with the environment and self-improve. Specifically, I've worked on:
- **Understanding the reason** that prevents LLM from effective self-improvement [[1]](https://arxiv.org/abs/2404.04298)
- **Building an environment** that provides faithful feedback for self-improvement
- **Incorporating reasoning techniques** to solve issues during self-improvement


The role of training data and alignment in reasoning and self-improvement has always fascinated me. I would like to explore this further when I have more time and resources.
