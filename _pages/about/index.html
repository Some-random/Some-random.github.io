<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>I am broadly interested in <strong>reasoning</strong>, which (IMHO) is a key aspect of human intelligence that sets us apart from other species. In the realm of reasoning, I’ve worked on:</p> <ul> <li> <strong>Building general-purpose verifier</strong> through rationale extraction from unlabelled data to provide process supervision during reasoning</li> <li> <strong>Logical reasoning</strong> that uses theorem prover <a href="https://lean-lang.org/" rel="external nofollow noopener" target="_blank">Lean</a> to help with the reasoning process <a href="https://arxiv.org/abs/2403.13312" rel="external nofollow noopener" target="_blank">[2]</a> </li> <li> <strong>Decompositional entailment</strong> that formulates a consistent and theoretically grounded approach to annotating decompositional entailment dataset <a href="https://arxiv.org/abs/2402.14798" rel="external nofollow noopener" target="_blank">[3]</a> </li> </ul> <p>I’m also interested in the <strong>self-improvement</strong> capability of LLMs. If we begin with the “end” (superintelligence/AGI) in mind, relying on human input won’t get us there. We need to teach models to interact with the environment and self-improve. Specifically, I’ve worked on:</p> <ul> <li> <strong>Understanding the reason</strong> that prevents LLM from effective self-improvement <a href="https://arxiv.org/abs/2404.04298" rel="external nofollow noopener" target="_blank">[1]</a> </li> <li> <strong>Building an environment</strong> that provides faithful feedback for self-improvement</li> <li> <strong>Incorporating reasoning techniques</strong> to solve issues during self-improvement</li> </ul> <p>The role of training data and alignment in reasoning and self-improvement has always fascinated me. I would like to explore this further when I have more time and resources.</p> </body></html>